# Analysis and Simulation of a Fair Queuing Algoirthm
What is your take-away message from this paper?
- Because the algorithm people choose to queue packets is determined by the user, FCFS is not an adequately fair queuing strategy. Instead, there needs to exist a different queuing algorithm that fairly allocates the network to different sources. This paper aims to prove that the new algorithm is fair under varying circumstances. What I took away from this paper is that algorithms shouldn't be designed in a manner where people are expected a certain behavior. Algorithms need to account for the worst action a user can do, and still achieve the goal that it is meant to achieve.

What is the motivation for this work (both people problem and technical problem), and its distillation into a research question? Why doesn’t the people problem have a trivial solution? What are the previous solutions and why are they inadequate?
- The motivation for this work is to introduce an algorithm that fixes fair queuing and prove that it works on real networks. This is hard because sources have no obligation to abide by a certain algorithm, so this algorithm needs to perform well despite that. It is also hard to simulate how real networks would work. Previous solutions didn't work because they didn't account for the fact that people would try to take more than their fair share.

What is the proposed solution (hypothesis, idea, design)? Why is it believed it will work? How does it represent an improvement? How is the solution achieved?
- The proposed solution is to use a round robin queuing on the gateway where no matter how many packets are sent to the gateway by sources each source gets a faire share of times the packet is processed. To guarantee fair share of bandwidth whenever a packet is sent then the next packet is the one with the smallest value finish time. This way large packets are sent last and thus it is incentivized for sources to send smaller packets. This simulates bit by bit round robin without fragmenting packets and prevents greedy sources from monopolizing bandwidth. This is an improvement over FIFO because FIFO has no way of actually ensuring fairness and greedy sources can easily take advantage of said algorithm

What is the author’s evaluation of the solution? What logic, argument, evidence, artifacts (e.g., a proof-of-concept system), or experiments are presented in support of the idea?
- The author says that the queuing delay of packets is on average uniform across all sources. This is confirmed through various scenarios in simulations and the numbers of packets dropped by supposed losers are consistent among all the sources. Through experimentation and looking at how often packets were dropped, it can be seen that the algorithm works as intended.

What is your analysis of the identified problem, idea and evaluation? Is this a good idea? What flaws do you perceive in the work? What are the most interesting or controversial ideas? For work that has practical implications, ask whether this will work, who would want it, what it will take to give it to them, and when might it become a reality?
- I think the analysis in the paper is correct, and I largely agree with the point made at the end where some sources DO need more bandwidth. While it is often fairi to want fair allocation of a network for all sources, there should be a bonus given to soruces that are more useful and need more bandwidth. I think another issue is that people can overcome this algorithm by creating multiple sources and split up the packets they want processed. There is no way for the source to know if sources are cooperating, so a netwokr can easily be unfair if this were to happen. Overall though I thought the paper was well thought out and accomplished what it could.

What are the paper’s contributions (author’s and your opinion)? Ideas, data structures, methods/algorithms, software, experimental results, experimental techniques...?
- The paper's contribution is very obvious in that it provided a new fair way to queue packets, such that sources are not competing with each other. The idea of assuming the worst of an adversary is good, and the way the experiments were tested was also smart. I also thought the paper was very rigorous in defending how fairness and allocation was defined.

What are future directions for this research (author’s and yours, perhaps driven by shortcomings or other critiques)?
- Like kind of noted in the paper I think some future direction for this research is giving priority to sources that need more bandwidth or less priority to sources that explicitly state they don't need as much bandwidth. This way sources that don't really care can lend their share to others. I also think including ECN in this algorithm would be beneficial, as it would tell senders how congested the network actually is at the moment.

Q8 What questions are you left with? What questions would you like to raise in an open discussion of the work (review interesting and controversial points, above)? What do you find difficult to understand? List as many as you can, say at least three, not including questions that can be answered quickly by searching the internet.
- For applications that require more latency or throughput how are these requirements adjusted by the fair policy? For example, if a source just sent a ton of small packets, then wouldn't it always get access first? Is this a price the queueing algorithm is okay with paying?
- How does the algorithm scale as gateways have to keep state for millions of flows?
- Should fairness be per application or per flow based?